{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fatest path data generator\n",
    "\n",
    "given generate num, grid size, car number(total_trajectories)\n",
    "generate random OD\n",
    "get capacity (weighted graph)\n",
    "generate fasted route according to OD and capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate trajectories for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# random simulation time\n",
    "simulation_num = 2000000\n",
    "test_simulation_num = 200\n",
    "# Define the grid size\n",
    "grid_size = 10\n",
    "# Define the total number of trajectories\n",
    "total_trajectories = 1\n",
    "capacity_scale = 10\n",
    "weight_quantization_scale = None # quantization scale for the weights, and in the network turn into embedding, None means no quantization\n",
    "max_connection = 4 # maximum number of connections for each node\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "def get_capacity(capacity_scale = 10, grid_size = 10):\n",
    "    grid_capacity = np.random.uniform(1,capacity_scale,grid_size*grid_size)\n",
    "    return grid_capacity\n",
    "\n",
    "def get_adjacency(grid_size=10):\n",
    "    adj = np.zeros([grid_size*grid_size,grid_size*grid_size],dtype= int)\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if i>0:\n",
    "                adj[(i-1)*grid_size+j,i*grid_size+j] = 1\n",
    "            if i<grid_size-1:\n",
    "                adj[(i+1)*grid_size+j,i*grid_size+j] = 1\n",
    "            if j>0:\n",
    "                adj[i*grid_size+j-1,i*grid_size+j] = 1\n",
    "            if j<grid_size-1:\n",
    "                adj[i*grid_size+j+1,i*grid_size+j] = 1\n",
    "    return adj\n",
    "\n",
    "def get_length():\n",
    "    return np.ones(grid_size*grid_size,dtype= int)\n",
    "\n",
    "# get weighted adjacency matrix and adjacency table\n",
    "def get_weighted_adjacency(adj, grid_capacity, length, normalization = True, quantization_scale = None, max_connection = 4):\n",
    "    weighted_adj_matrix = copy.deepcopy(adj)\n",
    "    adj_table = np.zeros([weighted_adj_matrix.shape[0],max_connection, 2]) # [node, connection, [target_node, weight]]\n",
    "    for i in range(len(adj)):\n",
    "        for j in range(len(adj)):\n",
    "            if adj[i,j] == 1:\n",
    "                weighted_adj_matrix[i,j] = (grid_capacity[j]+1)*length[j]\n",
    "                adj_table[i,np.sum(adj_table[i,:,0]!=0)] = [j,weighted_adj_matrix[i,j]] # [target_node, weight], add to the first empty slot\n",
    "    if normalization:\n",
    "        weighted_adj = weighted_adj_matrix/np.max(weighted_adj_matrix)\n",
    "        adj_table[:,:,1] = adj_table[:,:,1]/np.max(adj_table[:,:,1])\n",
    "    if quantization_scale:\n",
    "        weighted_adj = np.ceil(weighted_adj*quantization_scale)\n",
    "        adj_table[:,:,1] = np.ceil(adj_table[:,:,1]*quantization_scale)\n",
    "        \n",
    "    return weighted_adj, adj_table\n",
    "\n",
    "\n",
    "\n",
    "# transfer node, wrighted_adj to graph\n",
    "def transfer_graph(weighted_adj, grid_size):\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(grid_size*grid_size):\n",
    "        G.add_node(i)\n",
    "    for i in range(grid_size*grid_size):\n",
    "        for j in range(grid_size*grid_size):\n",
    "            if weighted_adj[i,j] != 0:\n",
    "                G.add_edge(i,j,weight=weighted_adj[i,j])\n",
    "    return G\n",
    "\n",
    "\n",
    "# get shortest traj\n",
    "def generate_trajectory_list(OD_list, weighted_adj):\n",
    "    trajectory_list = []\n",
    "    G = transfer_graph(weighted_adj, grid_size)\n",
    "    for i in range(len(OD_list)):\n",
    "        trajectory = nx.shortest_path(G, (OD_list[i][0][0]-1)*grid_size+OD_list[i][0][1]-1, (OD_list[i][1][0]-1)*grid_size+OD_list[i][1][1]-1, weight='weight')\n",
    "        for j in range(len(trajectory)):\n",
    "            trajectory[j] = (trajectory[j]//grid_size+1,trajectory[j]%grid_size+1)\n",
    "        trajectory_list.append(trajectory)\n",
    "    return trajectory_list\n",
    "        \n",
    "\n",
    "def get_OD_list(grid_size=10, trajectory_num = 10):\n",
    "    # define OD [trajectory_num,2,2] (N*[[x_start,y_start],[x_end,y_end]])\n",
    "    # 1-indexing\n",
    "    OD_list = np.random.randint(1,grid_size+1,[trajectory_num,2,2])\n",
    "    return OD_list\n",
    "\n",
    "\n",
    "# Generate the codebook\n",
    "codebook = {}\n",
    "max_value = grid_size  # max(y) = 10 in this case\n",
    "\n",
    "# Populate the codebook with grid cells\n",
    "for x in range(1, grid_size + 1):\n",
    "    for y in range(1, grid_size + 1):\n",
    "        code = (x - 1) * max_value + y\n",
    "        codebook[(x, y)] = str(code)\n",
    "codebook[(0,0)] = f'{grid_size*grid_size+1}'\n",
    "codebook[(grid_size+1,grid_size+1)] = f'{grid_size*grid_size+2}'\n",
    "\n",
    "# Save the codebook to a file\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "with open('./data/codebook.txt', 'w') as file:\n",
    "    for key, value in codebook.items():\n",
    "        file.write(f'{key}: {value}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#simulation for simulation_num times\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "all_trajectory_list = []\n",
    "all_weighted_adj_list = []\n",
    "all_adj_table_list = []\n",
    "\n",
    "if simulation_num != 0:\n",
    "    for t in range(simulation_num):\n",
    "        # print(f'simulation {t}')\n",
    "        # Generate and save 10 trajectories\n",
    "        all_encoded_trajectories = []\n",
    "\n",
    "        OD_list = get_OD_list(grid_size=grid_size, trajectory_num= total_trajectories)\n",
    "        # grid_capacity = get_capacity(current_trajectory_list=OD_list[:,0,:],grid_size=grid_size)\n",
    "        grid_capacity = get_capacity(capacity_scale,grid_size)\n",
    "        adj = get_adjacency(grid_size=grid_size)\n",
    "        road_len = get_length()\n",
    "        weighted_adj, adj_table = get_weighted_adjacency(adj=adj,grid_capacity=grid_capacity,length=road_len, normalization=True, quantization_scale=weight_quantization_scale, max_connection = max_connection)\n",
    "        trajectory_list = generate_trajectory_list(OD_list, weighted_adj)\n",
    "\n",
    "        for i in range(total_trajectories):\n",
    "            trajectory = trajectory_list[i]\n",
    "            encoded_trajectory = [codebook[(x, y)] for x, y in trajectory]\n",
    "            # # Append '0' at the end of each trajectory\n",
    "            # encoded_trajectory.append('0')\n",
    "            all_encoded_trajectories.append(encoded_trajectory)\n",
    "\n",
    "        # Save all trajectories to a single file\n",
    "        # with open(f'./data/trajectory_{t}.txt', 'w') as file:\n",
    "        #     # file.write(' '.join(all_encoded_trajectories))\n",
    "        #     for trajectory in all_encoded_trajectories:\n",
    "        #         file.write(' '.join(trajectory))\n",
    "        #         file.write('\\n')\n",
    "        # np.save(f'./data/weighted_adjacency_{t}.npy', weighted_adj)\n",
    "        # np.save(f'./data/adj_table_{t}.npy', adj_table)\n",
    "        print(f'simulation {t}')\n",
    "\n",
    "        # Save all trajectories to a single file\n",
    "        all_trajectory_list.append(all_encoded_trajectories)\n",
    "        all_weighted_adj_list.append(weighted_adj)\n",
    "        all_adj_table_list.append(adj_table)\n",
    "        print(f'simulation {t}')\n",
    "\n",
    "    with open('./data/trajectory_list.pkl', 'wb') as file:\n",
    "        pickle.dump(all_trajectory_list, file)\n",
    "    with open('./data/weighted_adj_list.pkl', 'wb') as file:\n",
    "        pickle.dump(all_weighted_adj_list, file)\n",
    "    with open('./data/adj_table_list.pkl', 'wb') as file:\n",
    "        pickle.dump(all_adj_table_list, file)\n",
    "\n",
    "    del all_trajectory_list, all_weighted_adj_list, all_adj_table_list\n",
    "    \n",
    "    print('Data saved successfully!')\n",
    "\n",
    "    # Plotting the trajectory\n",
    "    for i in range(total_trajectories):\n",
    "        trajectory = []\n",
    "        for j in range(len(trajectory_list[i])):\n",
    "            x, y = trajectory_list[i][j]\n",
    "            if x == 0 and y == 0:\n",
    "                continue\n",
    "            elif x == grid_size + 1 and y == grid_size + 1:\n",
    "                break\n",
    "            trajectory.append((x, y))\n",
    "        trajectory_x, trajectory_y = zip(*trajectory)\n",
    "        plt.plot(trajectory_x, trajectory_y, marker='o', label=f'Trajectory {i+1}')\n",
    "\n",
    "    # Plot configuration\n",
    "    plt.xlim(0, grid_size + 1)\n",
    "    plt.ylim(0, grid_size + 1)\n",
    "    plt.xticks(np.arange(1, grid_size + 1, 1))\n",
    "    plt.yticks(np.arange(1, grid_size + 1, 1))\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(f'total {len(trajectory_list[-1])} steps')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1. Define Functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# the X should be (B, N, L, C), B is batch size, N is the number of car, L is the max trajectory length, C is the embedding channel\n",
    "# adj should be (V,V)\n",
    "\n",
    "class NormalizedEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return x/torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def get_1d_sincos_geo_embed(d_cross, pos):\n",
    "    \"\"\"\n",
    "    d_cross: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (V,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert d_cross % 2 == 0\n",
    "    omega = np.arange(d_cross // 2, dtype=np.float64)\n",
    "    omega /= d_cross / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (V,)\n",
    "    out = np.einsum('v,d->vd', pos, omega)  # (V, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (V, D/2)\n",
    "    emb_cos = np.cos(out)  # (V, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=-1)  # (V, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_2d_sincos_geo_embed(emb_1d):\n",
    "    \"\"\"\n",
    "    1d_emb: (V, D)\n",
    "    out: (V, V, D)\n",
    "    \"\"\"\n",
    "    emb_1d = emb_1d.reshape(-1, emb_1d.shape[-1])  # (V, D)\n",
    "    emb_2d = np.einsum('hd,wd->hwd', emb_1d, emb_1d)  # (V, V, D)\n",
    "    # print(emb_2d)\n",
    "    return emb_2d\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, block_size, n_embd, dropout=0.1, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=in_proj_bias)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=in_proj_bias)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(head_size, head_size, bias=out_proj_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        input_shape = x.shape\n",
    "        batch_size, trajectory_num, sequence_length, head_size = input_shape\n",
    "\n",
    "        # (batch_size, trajectory_num, sequence_length, head_size)\n",
    "        k = self.key(x)\n",
    "        # (batch_size, trajectory_num, sequence_length, head_size)\n",
    "        q = self.query(x)\n",
    "        # (batch_size, trajectory_num, sequence_length, head_size)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # (B*N, T, T)\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "        weight /= math.sqrt(head_size)\n",
    "        weight = torch.masked_fill(weight, mask, value=-1e7)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        output = weight @ v\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, block_size, n_embd, dropout=dropout\n",
    "                                         ) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout=0.1, norm_position='prenorm'):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.norm_position = norm_position\n",
    "        self.sa = MultiHeadAttention(\n",
    "            n_head, head_size, n_embd, block_size, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.norm_position == 'prenorm':\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "        else:\n",
    "            x = self.ln1(x + self.sa(x))\n",
    "            x = self.ln2(x + self.ffwd(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads: int, n_hidden: int, n_embd: int, dropout=0.1, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_proj = nn.Linear(n_hidden, n_hidden, bias=in_proj_bias)\n",
    "        self.k_proj = nn.Linear(n_embd, n_hidden, bias=in_proj_bias)\n",
    "        self.v_proj = nn.Linear(n_embd, n_hidden, bias=in_proj_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(n_hidden, n_hidden, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = n_hidden // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor):\n",
    "        # x: (Traj) (B, N, L, H)\n",
    "        # if use matrix adj: (Road) (B, V, V, C)\n",
    "        # if use table adj: (B, V, E, C), same process as the matrix adj\n",
    "\n",
    "        input_shape = x.shape\n",
    "        batch_size, trajectory_num, sequence_length, n_embd = input_shape\n",
    "\n",
    "        interm_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "\n",
    "        q = self.q_proj(x)  # (B, N, L, H) -> (B, N, L, H)\n",
    "        k = self.k_proj(adj)  # (B, V, V, C) -> (B, V, V, H)\n",
    "        v = self.v_proj(adj)  # (B, V, V, C) -> (B, V, V, H)\n",
    "\n",
    "        # (B, N, L, H) -> (B, N*L, n_heads, d_head) -> (B, n_heads, N*L, d_head)\n",
    "        q = q.view(interm_shape).transpose(1, 2)\n",
    "        # (B, V, V, H) -> (B, V*V, n_heads, d_head) -> (B, n_heads, V*V, d_head)\n",
    "        k = k.view(interm_shape).transpose(1, 2)\n",
    "        v = v.view(interm_shape).transpose(1, 2)\n",
    "\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "        weight /= math.sqrt(self.d_head)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        # (B, n_heads, N*L, V*V) @ (B, n_heads, V*V, d_head) -> (B, n_heads, N*L, d_head)\n",
    "        output = weight @ v\n",
    "\n",
    "        # (B, n_heads, N*L, d_head) -> (B, N*L, n_heads, d_head)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # (B, N*L, n_heads, d_head) -> (B, N, L, H)\n",
    "        output = output.view(input_shape)\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        # (B, N, L, H)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads: int, n_hidden: int, n_embd: int, dropout=0.1, in_proj_bias=True, out_proj_bias=True, norm_position='prenorm'):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = CrossAttention(\n",
    "            n_heads, n_hidden, n_embd, dropout, in_proj_bias, out_proj_bias)\n",
    "        self.ffd = FeedFoward(n_hidden, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_hidden)\n",
    "        self.ln2 = nn.LayerNorm(n_hidden)\n",
    "        self.norm_position = norm_position\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # x: (B, N, L, H)\n",
    "        # adj: (B, V, V, C)\n",
    "        if self.norm_position == 'prenorm':\n",
    "            x = x + self.ln1(self.att(x, adj))\n",
    "            x = x + self.ln2(self.ffd(x))\n",
    "        else:\n",
    "            x = self.ln1(x + self.att(x, adj))\n",
    "            x = self.ln2(x + self.ffd(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class no_diffusion_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, n_embd: int, n_hidden: int, n_layer: int, n_head: int, block_size: int,\n",
    "                 dropout=0.1,\n",
    "                 weight_quantization_scale: Optional[int] = None,\n",
    "                 use_adj_table=True,\n",
    "                 use_ne=True,\n",
    "                 use_ge=False,\n",
    "                 use_agent_mask=False,\n",
    "                 norm_position='prenorm',\n",
    "                 device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        if use_ne:\n",
    "            self.token_embedding_table = NormalizedEmbedding(\n",
    "                vocab_size, n_embd)\n",
    "        else:\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        if weight_quantization_scale:\n",
    "            if use_ne:\n",
    "                self.weight_embedding_table = NormalizedEmbedding(\n",
    "                    weight_quantization_scale+1, n_embd)\n",
    "            else:\n",
    "                self.weight_embedding_table = nn.Embedding(\n",
    "                    weight_quantization_scale+1, n_embd)\n",
    "            # +1 because 0 means no edge\n",
    "        else:\n",
    "            self.adj_embed = nn.Sequential(\n",
    "                nn.Linear(1, 2*vocab_size),\n",
    "                nn.LayerNorm(2*vocab_size),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(2*vocab_size, n_embd),\n",
    "            )\n",
    "\n",
    "        # Geolocation embedding\n",
    "        # (B, V, V, n_embd)\n",
    "        # to demonstrate the end, we add 0 to the rest of trajectory, so the vocab_size = V + 1\n",
    "        if use_adj_table:\n",
    "            if use_ge:\n",
    "                self.geolocation_embedding = torch.from_numpy(get_1d_sincos_geo_embed(\n",
    "                    n_embd, np.arange(1, vocab_size)),\n",
    "                ).to(device).float().unsqueeze(0).unsqueeze(2) # (1, V, 1, n_embd)\n",
    "            else:\n",
    "                self.geolocation_embedding = torch.zeros(\n",
    "                    (1, vocab_size-1, 1, n_embd), device=device)\n",
    "        else:\n",
    "            if use_ge:\n",
    "                self.geolocation_embedding = torch.from_numpy(get_2d_sincos_geo_embed(get_1d_sincos_geo_embed(\n",
    "                    n_embd, np.arange(1, vocab_size)),\n",
    "                )).to(device).float().unsqueeze(0) # (1, V, V, n_embd)\n",
    "            else:\n",
    "                self.geolocation_embedding = torch.zeros(\n",
    "                    (1, vocab_size-1, vocab_size-1, n_embd), device=device)\n",
    "\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_hidden),\n",
    "            nn.LayerNorm(n_hidden),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([CrossAttentionBlock(\n",
    "            n_head, n_hidden, n_embd, dropout, norm_position=norm_position) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_hidden)  # final layer norm\n",
    "        self.lm_head = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "        self.condition_proj = nn.Sequential(\n",
    "            nn.Linear(n_embd*3, n_hidden),\n",
    "            nn.LayerNorm(n_hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(n_hidden, 3),\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "        self.use_agent_mask = use_agent_mask\n",
    "        self.use_adj_table = use_adj_table\n",
    "        self.weight_quantization_scale = weight_quantization_scale\n",
    "\n",
    "    def forward(self, condition: torch.Tensor, weighted_adj: torch.Tensor, y: Optional[torch.Tensor] = None,\n",
    "                agent_mask: Optional[torch.Tensor] = None, special_mask: Optional[torch.Tensor] = None):\n",
    "        # Input:\n",
    "        # condition: (B, N, 2)\n",
    "        # weighted_adj: (B, V, V) or (B, V, E, 2)\n",
    "        # y: (B, N, L)\n",
    "        # adjmask: (B, V, V)\n",
    "        # special mask: (B, N, L)\n",
    "        # Output: (B, N, L)\n",
    "\n",
    "        B, N, _ = condition.shape\n",
    "        L = self.block_size\n",
    "        x = torch.zeros((B, N, L), device=self.device).int()\n",
    "        # put the origin into the first position\n",
    "        x[:, :, 0] = condition[:, :, 0]\n",
    "\n",
    "        if not self.use_agent_mask:\n",
    "            agent_mask = None\n",
    "\n",
    "        # x and y are both (B, N, L) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(x)  # (B, N, L ,C)\n",
    "\n",
    "        pos_emb = self.position_embedding_table(torch.arange(\n",
    "            L, device=self.device)).view(1, 1, L, -1)  # (1,1,L,C)\n",
    "\n",
    "        if self.use_adj_table:\n",
    "            if self.weight_quantization_scale:\n",
    "                adj = self.token_embedding_table(weighted_adj[:, :, :, 0].int()) + self.weight_embedding_table(weighted_adj[:, :, :, 1].int()) + self.geolocation_embedding\n",
    "            else:\n",
    "                adj = self.token_embedding_table(weighted_adj[:, :, :, 0].int())+ self.adj_embed(weighted_adj[:, :, :, 1].unsqueeze(-1)) + self.geolocation_embedding\n",
    "        else:\n",
    "            if self.weight_quantization_scale:\n",
    "                adj = self.weight_embedding_table(weighted_adj.int()) + self.geolocation_embedding\n",
    "            else:\n",
    "                adj = self.adj_embed(weighted_adj.unsqueeze(-1)) + self.geolocation_embedding\n",
    "\n",
    "        if condition is not None:\n",
    "            # broadcastTensor = torch.zeros((B,T,N,2)).to(self.device).long()\n",
    "            # condition = condition + broadcastTensor\n",
    "            # TODO how to add condition to the input\n",
    "            condition_s = condition[:, :, 0]  # (B, N)\n",
    "            condition_e = condition[:, :, 1]  # (B, N)\n",
    "            # (B, N, L)\n",
    "            condition_s = condition_s.unsqueeze(-1).expand(B, N, L)\n",
    "            # (B, N, L)\n",
    "            condition_e = condition_e.unsqueeze(-1).expand(B, N, L)\n",
    "            condition_s_emb = self.token_embedding_table(\n",
    "                condition_s.long())  # (B, N, L, C)\n",
    "            condition_e_emb = self.token_embedding_table(\n",
    "                condition_e.long())  # (B, N, L, C)\n",
    "            condition_emb = torch.cat(\n",
    "                (tok_emb, condition_s_emb, condition_e_emb), dim=-1)  # (B, N, L, 3C)\n",
    "            condition_score = torch.softmax(self.condition_proj(\n",
    "                condition_emb), dim=-1)  # (B, N, L, 3)\n",
    "            condition_emb = torch.einsum(\n",
    "                # (B, N, L, C)\n",
    "                'bnld,bnldc->bnlc', condition_score, condition_emb.view(B, N, L, 3, -1))\n",
    "        else:\n",
    "            condition_emb = 0\n",
    "\n",
    "        x = tok_emb + pos_emb + condition_emb  # (B,N,L,C)\n",
    "        x = self.in_proj(x)\n",
    "        # x = self.blocks(x) # (B,T,N,C)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, adj)\n",
    "\n",
    "        x = self.ln_f(x)  # (B,N,L,C)\n",
    "        logits = self.lm_head(x)  # (B,N,L,V)\n",
    "\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            if special_mask is None:\n",
    "                special_mask = torch.ones_like(y).float()\n",
    "            B, N, L, V = logits.shape\n",
    "            logits_ = logits.view(B*N*L, V)\n",
    "            y = y.view(B*N*L)\n",
    "            special_mask = special_mask.view(B*N*L)\n",
    "            if agent_mask is not None:\n",
    "                mask_weight = agent_mask.view(B*L*N)\n",
    "                loss = (F.cross_entropy(logits_, y, reduction='none')\n",
    "                        * special_mask*mask_weight).sum()/mask_weight.sum()/special_mask.sum()\n",
    "            else:\n",
    "                loss = (F.cross_entropy(logits_, y, reduction='none')\n",
    "                        * special_mask).sum()/special_mask.sum()\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def test(self, condition: torch.Tensor, weighted_adj: torch.Tensor, y: Optional[torch.Tensor] = None,\n",
    "             agent_mask: Optional[torch.Tensor] = None, special_mask: Optional[torch.Tensor] = None):\n",
    "        logits, _ = self.forward(\n",
    "            condition, weighted_adj, y, agent_mask, special_mask)\n",
    "        # (B, N, L, V) -> (B, N, L)\n",
    "        logits = torch.argmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "# HERE SELECTING WHETHER TO USE FINITE SIMULATION DATA OR TO GENERATE NEW DATA\n",
    "use_given_data = True\n",
    "if simulation_num == 0:\n",
    "    use_given_data = False\n",
    "\n",
    "# Interations for training model\n",
    "max_iters = 20000\n",
    "learning_rate = 1e-1\n",
    "eval_iters = int(max_iters/100)\n",
    "# eval_iters = 1\n",
    "save_iters = 10000\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 1024 # how many independent djkastra graph will we process in parallel?\n",
    "test_size = 64\n",
    "path_num = 1 # on each weighted graph, how many shortest path will we consider?\n",
    "block_size = 25 # The max length of all shortest path\n",
    "special_mask_value = 0.01 # The value of special mask\n",
    "\n",
    "n_embd = 64\n",
    "n_head = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "# ------------\n",
    "\n",
    "\n",
    "# Function to read the encoded data from a file and save it as a list of integers\n",
    "def read_encoded_trajectory(filename, block_size = 10):\n",
    "    all_encoded_trajectories = []\n",
    "    all_condition = []\n",
    "    all_special_mask = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            trajectory = line.strip().split()\n",
    "            trajectory = [int(code) for code in trajectory]\n",
    "            condition = [trajectory[0],trajectory[-1]]\n",
    "            special_mask = np.ones(block_size)\n",
    "            if len(trajectory) > block_size:\n",
    "                raise ValueError(f'Trajectory length {len(trajectory)} is greater than block size {block_size}')\n",
    "            elif len(trajectory) < block_size:\n",
    "                special_mask[len(trajectory)+1:] = 0\n",
    "                trajectory += [0] * (block_size - len(trajectory))\n",
    "            all_encoded_trajectories.append(trajectory)\n",
    "            all_condition.append(condition)\n",
    "            all_special_mask.append(special_mask)\n",
    "\n",
    "    # all_encoded_trajectories: NxT, T can be different for each trajectory\n",
    "    # all_condition: Nx2\n",
    "    # all_special_mask: NxT\n",
    "    return all_encoded_trajectories, all_condition, all_special_mask\n",
    "\n",
    "\n",
    "def refine_trajectory(trajectory,block_size):\n",
    "    all_encoded_trajectories = []\n",
    "    all_condition = []\n",
    "    all_special_mask = []\n",
    "    for i in range(len(trajectory)):\n",
    "        traj = trajectory[i]\n",
    "        traj = [int(code) for code in traj]\n",
    "        condition = [traj[0],traj[-1]]\n",
    "        special_mask = np.ones(block_size)\n",
    "        if len(traj) > block_size:\n",
    "            raise ValueError(f'Trajectory length {len(traj)} is greater than block size {block_size}')\n",
    "        elif len(traj) < block_size:\n",
    "            special_mask[len(traj)+1:] = 0\n",
    "            traj += [0] * (block_size - len(traj))\n",
    "        all_encoded_trajectories.append(traj)\n",
    "        all_condition.append(condition)\n",
    "        all_special_mask.append(special_mask)\n",
    "    \n",
    "    return all_encoded_trajectories, all_condition, all_special_mask\n",
    "    \n",
    "\n",
    "def read_adjacency(filename):\n",
    "    return torch.tensor(np.load(filename))\n",
    "\n",
    "def read_data_list(simulation_num, block_size):\n",
    "    print('Loading the encoded trajectory...')\n",
    "    time_ = time.time()\n",
    "    trajectory_list = []\n",
    "    weighted_adj_list = []\n",
    "    adj_table_list = []\n",
    "    condition_list = []\n",
    "    special_mask_list = []\n",
    "    for i in range(simulation_num):\n",
    "        all_encoded_trajectories, all_condition, all_special_mask = read_encoded_trajectory(f'./data/trajectory_{i}.txt', block_size=block_size)\n",
    "        all_weighted_adj = read_adjacency(f'./data/weighted_adjacency_{i}.npy')\n",
    "        all_adj_table = read_adjacency(f'./data/adj_table_{i}.npy')\n",
    "        trajectory_list.append(all_encoded_trajectories)\n",
    "        weighted_adj_list.append(all_weighted_adj)\n",
    "        adj_table_list.append(all_adj_table)\n",
    "        condition_list.append(all_condition)\n",
    "        special_mask_list.append(all_special_mask)\n",
    "\n",
    "    assert len(trajectory_list) == simulation_num and len(weighted_adj_list) == simulation_num and len(condition_list) == simulation_num\n",
    "    print('Encoded trajectory loaded, time:', time.time()-time_)\n",
    "    # trajectory_list: [simulation_num, trajectory_num, block_size]\n",
    "    # weighted_adj_list: [simulation_num, grid_size*grid_size, grid_size*grid_size]\n",
    "    # condition_list: [simulation_num, trajectory_num, 2]\n",
    "    # special_mask_list: [simulation_num, trajectory_num, block_size]\n",
    "    return trajectory_list, weighted_adj_list, adj_table_list, condition_list, special_mask_list\n",
    "\n",
    "\n",
    "def read_data_pkl(simulation_num, block_size, root = './data'):\n",
    "    time_ = time.time()\n",
    "    trajectory_list = []\n",
    "    weighted_adj_list = []\n",
    "    adj_table_list = []\n",
    "    condition_list = []\n",
    "    special_mask_list = []\n",
    "    \n",
    "    with open(f'{root}/trajectory_list.pkl', 'rb') as file:\n",
    "        all_trajectory_list = pickle.load(file)\n",
    "    with open(f'{root}/weighted_adj_list.pkl', 'rb') as file:\n",
    "        all_weighted_adj_list = pickle.load(file)\n",
    "    with open(f'{root}/adj_table_list.pkl', 'rb') as file:\n",
    "        all_adj_table_list = pickle.load(file)\n",
    "\n",
    "    print(simulation_num)\n",
    "    for i in range(simulation_num):\n",
    "        all_encoded_trajectories, all_condition, all_special_mask = refine_trajectory(all_trajectory_list[i], block_size=block_size)\n",
    "        trajectory_list.append(all_encoded_trajectories)\n",
    "        condition_list.append(all_condition)\n",
    "        special_mask_list.append(all_special_mask)\n",
    "    weighted_adj_list = all_weighted_adj_list\n",
    "    adj_table_list = all_adj_table_list\n",
    "\n",
    "    assert len(trajectory_list) == simulation_num and len(weighted_adj_list) == simulation_num and len(condition_list) == simulation_num\n",
    "    print('Encoded trajectory loaded, time:', time.time()-time_)\n",
    "    # trajectory_list: [simulation_num, trajectory_num, block_size]\n",
    "    # weighted_adj_list: [simulation_num, grid_size*grid_size, grid_size*grid_size]\n",
    "    # condition_list: [simulation_num, trajectory_num, 2]\n",
    "    # special_mask_list: [simulation_num, trajectory_num, block_size]\n",
    "    return trajectory_list, weighted_adj_list, adj_table_list, condition_list, special_mask_list\n",
    "\n",
    "\n",
    "def generate_new_data(grid_size=10,trajectory_num = 1, block_size = 10):\n",
    "    OD_list = get_OD_list(grid_size=grid_size, trajectory_num=trajectory_num)\n",
    "    grid_capacity = get_capacity(grid_size=grid_size)\n",
    "    adj = get_adjacency(grid_size=grid_size)\n",
    "    road_len = get_length()\n",
    "    weighted_adj, adj_table = get_weighted_adjacency(adj=adj,grid_capacity=grid_capacity,length=road_len, normalization=True, quantization_scale=weight_quantization_scale, max_connection = max_connection)\n",
    "    trajectory_list = generate_trajectory_list(OD_list, weighted_adj)\n",
    "\n",
    "    all_encoded_trajectories = []\n",
    "    all_condition = []\n",
    "    all_special_mask = []\n",
    "    for i in range(trajectory_num):\n",
    "        trajectory = trajectory_list[i]\n",
    "        encoded_trajectory = [int(codebook[(x, y)]) for x, y in trajectory]\n",
    "        condition = [encoded_trajectory[0],encoded_trajectory[-1]]\n",
    "        special_mask = np.ones(block_size)\n",
    "        if len(encoded_trajectory) > block_size:\n",
    "            raise ValueError(f'Trajectory length {len(encoded_trajectory)} is greater than block size {block_size}')\n",
    "        elif len(encoded_trajectory) < block_size:\n",
    "            special_mask[len(encoded_trajectory)+1:] = 0\n",
    "            encoded_trajectory += [0] * (block_size - len(encoded_trajectory))\n",
    "        all_encoded_trajectories.append(encoded_trajectory)\n",
    "        all_condition.append(condition)\n",
    "        all_special_mask.append(special_mask)\n",
    "\n",
    "    # all_encoded_trajectories: [trajectory_num, block_size]\n",
    "    # all_weighted_adj: [grid_size*grid_size, grid_size*grid_size]\n",
    "    # all_condition: [trajectory_num, 2]\n",
    "    # all_special_mask: [trajectory_num, block_size]\n",
    "    return all_encoded_trajectories, weighted_adj, adj_table, all_condition, all_special_mask\n",
    "\n",
    "def generate_data_list(simulation_num,total_trajectories, block_size):\n",
    "    trajectory_list = []\n",
    "    weighted_adj_list = []\n",
    "    adj_table_list = []\n",
    "    condition_list = []\n",
    "    special_mask_list = []\n",
    "    # print('Generating new encoded trajectory...')\n",
    "    time_ = time.time()\n",
    "    for i in range(simulation_num):\n",
    "        all_encoded_trajectories, weighted_adj, adj_table, all_condition, all_special_mask = generate_new_data(grid_size=grid_size,trajectory_num = total_trajectories, block_size=block_size)\n",
    "        trajectory_list.append(all_encoded_trajectories)\n",
    "        weighted_adj_list.append(weighted_adj)\n",
    "        adj_table_list.append(adj_table)\n",
    "        condition_list.append(all_condition)\n",
    "        special_mask_list.append(all_special_mask)\n",
    "    assert len(trajectory_list) == simulation_num and len(weighted_adj_list) == simulation_num and len(condition_list) == simulation_num\n",
    "    # print('New encoded trajectory generated, time:', time.time()-time_)\n",
    "\n",
    "    # trajectory_list: [simulation_num, trajectory_num, block_size]\n",
    "    # weighted_adj_list: [simulation_num, grid_size*grid_size, grid_size*grid_size]\n",
    "    # condition_list: [simulation_num, trajectory_num, 2]\n",
    "    return trajectory_list, weighted_adj_list, adj_table_list, condition_list, special_mask_list\n",
    "    \n",
    "# dataloaders\n",
    "class data_loader():\n",
    "    def __init__(self, use_given_data = True, simulation_num = 400, block_size = 20):\n",
    "        self.simulation_num = simulation_num\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.use_given_data = use_given_data\n",
    "        if use_given_data:\n",
    "            self.trajectory_list, self.weighted_adj_list, self.adj_table_list, self.condition_list, self.special_mask_list = read_data_pkl(simulation_num, block_size)\n",
    "            self.index = 0\n",
    "            self.test_trajectory_list, self.test_weighted_adj_list, self.test_adj_table_list, self.test_condition_list, self.test_special_mask_list = read_data_pkl(test_simulation_num, block_size, root = './data_test')\n",
    "            self.index_test = 0\n",
    "        \n",
    "    def load_train_batch(self, batch_size = 32, total_trajectories = 1):\n",
    "        idx = self.index * batch_size % self.simulation_num\n",
    "        self.index += 1\n",
    "        # Get the trajectory and the weighted adjacency matrix\n",
    "        trajectory = self.trajectory_list[idx:idx+batch_size] # B, N, L\n",
    "        weighted_adj = self.weighted_adj_list[idx:idx+batch_size] # B, V, V\n",
    "        adj_table = self.adj_table_list[idx:idx+batch_size] # B, V, E, 2\n",
    "        condition = self.condition_list[idx:idx+batch_size] # B, N, 2\n",
    "        special_mask = self.special_mask_list[idx:idx+batch_size] # B, N, L\n",
    "        return torch.tensor(trajectory), torch.tensor(np.array(weighted_adj)).float(), torch.tensor(np.array(adj_table)).float(), torch.tensor(condition), torch.tensor(np.array(special_mask))\n",
    "    \n",
    "    def load_test_batch(self, batch_size = 32, total_trajectories = 1):\n",
    "        idx = self.index_test * batch_size % test_simulation_num\n",
    "        self.index_test += 1\n",
    "        # Get the trajectory and the weighted adjacency matrix\n",
    "        trajectory = self.test_trajectory_list[idx:idx+batch_size]\n",
    "        weighted_adj = self.test_weighted_adj_list[idx:idx+batch_size]\n",
    "        adj_table = self.test_adj_table_list[idx:idx+batch_size]\n",
    "        condition = self.test_condition_list[idx:idx+batch_size]\n",
    "        special_mask = self.test_special_mask_list[idx:idx+batch_size]\n",
    "        return torch.tensor(trajectory), torch.tensor(np.array(weighted_adj)).float(), torch.tensor(np.array(adj_table)).float(), torch.tensor(condition), torch.tensor(np.array(special_mask))\n",
    "    \n",
    "    def generate_batch(self, batch_size = 32, total_trajectories = 1):\n",
    "        trajectory, weighted_adj, adj_table, condition, special_mask = generate_data_list(batch_size, total_trajectories, self.block_size)\n",
    "        return torch.tensor(trajectory), torch.tensor(np.array(weighted_adj)).float(), torch.tensor(np.array(adj_table)).float(), torch.tensor(condition), torch.tensor(np.array(special_mask))\n",
    "\n",
    "# test function whether a trajectory skip a node\n",
    "def test_trajectory_error(pred, weighted_adj, trajectory):\n",
    "    i = 0\n",
    "    while pred[i] != 0 and pred[i+1] != 0 and i < len(pred)-1:\n",
    "        if weighted_adj[pred[i]-1,pred[i+1]-1] == 0: # currently the codebook is 1-indexing\n",
    "            return True\n",
    "        i += 1\n",
    "    j = 0 \n",
    "    while trajectory[j] != 0 and trajectory[j+1] != 0 and j < len(trajectory)-1:\n",
    "        j += 1\n",
    "    if pred[i] != trajectory[j]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Calculate the length of a trajectory\n",
    "def calculate_length(trajectory, weighted_adj):\n",
    "    length = 0\n",
    "    for i in range(len(trajectory)-1):\n",
    "        if trajectory[i+1] != 0:\n",
    "            length += weighted_adj[trajectory[i]-1,trajectory[i+1]-1] # currently the codebook is 1-indexing\n",
    "        else:\n",
    "            break\n",
    "    return length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "use_adj_table = True\n",
    "load_dir_id = None\n",
    "# load_dir_id = 10000\n",
    "continue_train = True\n",
    "# continue_train = False\n",
    "vocab_size = grid_size*grid_size+1 # 0-indexing, 0 means the end placeholder\n",
    "\n",
    "model= no_diffusion_model(vocab_size, n_embd, n_embd, n_layer, n_head, block_size, dropout, weight_quantization_scale, use_adj_table=use_adj_table, use_ne=True, use_ge=True, use_agent_mask=False, norm_position='prenorm', device=device)\n",
    "\n",
    "model = model.to(device)\n",
    "if load_dir_id:\n",
    "    model.load_state_dict(torch.load(f'./checkpoint/model_{load_dir_id}.pth'))\n",
    "    print('Model loaded from', f'./checkpoint/model_{load_dir_id}.pth')\n",
    "    learning_rate *= 0.5**int(max_iters/load_dir_id)\n",
    "\n",
    "loader = data_loader(use_given_data, simulation_num, block_size)\n",
    "if not os.path.exists('./checkpoint'):\n",
    "    os.makedirs('./checkpoint')\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-15)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-15)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=save_iters, gamma=0.5)\n",
    "# lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iters, eta_min=0)\n",
    "\n",
    "logger_train_loss = []\n",
    "logger_test_loss = []\n",
    "logger_test_error_rate = []\n",
    "logger_test_real_true_rate = []\n",
    "logger_test_fake_true_rate = []\n",
    "logger_test_length_rate_avg = []\n",
    "logger_test_leng_10_rate = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if continue_train:\n",
    "    start = 1\n",
    "    if load_dir_id:\n",
    "        start = load_dir_id+1\n",
    "    for i in range (start, max_iters+1):\n",
    "        model.train()\n",
    "        trajectory, weighted_adj, adj_table, condition, special_mask = loader.load_train_batch(batch_size)\n",
    "        trajectory = trajectory.to(device)\n",
    "        if use_adj_table:\n",
    "            adj = adj_table.to(device)\n",
    "        else:\n",
    "            adj = weighted_adj.to(device)\n",
    "        condition = condition.to(device)\n",
    "        special_mask = special_mask.to(device)\n",
    "        special_mask = (special_mask+special_mask_value).clamp(0,1).float()\n",
    "\n",
    "        logits, loss = model(condition, adj, trajectory, None, special_mask)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        logger_train_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "\n",
    "        if i % eval_iters == 0:\n",
    "            model.eval()\n",
    "            result_text = f'Iteration {i:<10}|  Loss: {loss.item():<10.8f}  |'\n",
    "            trajectory, weighted_adj, adj_table, condition, special_mask = loader.load_test_batch(test_size)\n",
    "            trajectory = trajectory.to(device)\n",
    "            if use_adj_table:\n",
    "                adj = adj_table.to(device)\n",
    "            else:\n",
    "                adj = weighted_adj.to(device)\n",
    "            condition = condition.to(device)\n",
    "            special_mask = special_mask.to(device)\n",
    "            special_mask_ = special_mask > 0 # record the bool mask for the special token\n",
    "            special_mask = (special_mask+special_mask_value).clamp(0,1).float()\n",
    "\n",
    "            logits, loss = model(condition, adj, trajectory, None, special_mask)\n",
    "            acc = ((torch.argmax(logits, dim=-1) == trajectory).float()*special_mask_).sum()/special_mask_.sum()\n",
    "\n",
    "            error_num = 0\n",
    "            real_true_num = 0\n",
    "            fake_true_num = 0\n",
    "            len_rate_sum = 0\n",
    "            len_10_num = 0\n",
    "            for j in range(trajectory.size(0)):\n",
    "                for k in range(trajectory.size(1)):\n",
    "                    pred = torch.argmax(logits, dim=-1)[j,k]\n",
    "                    if test_trajectory_error(pred, weighted_adj[j], trajectory[j,k]):\n",
    "                        error_num += 1\n",
    "                    elif calculate_length(trajectory[j,k], weighted_adj[j]) != 0:\n",
    "                        len_true = calculate_length(trajectory[j,k], weighted_adj[j])\n",
    "                        len_pred = calculate_length(pred, weighted_adj[j])\n",
    "                        len_rate = len_pred/len_true\n",
    "                        len_rate_sum += len_rate\n",
    "                        if len_true == len_pred:\n",
    "                            if (torch.argmax(logits, dim=-1)[j,k] == trajectory[j,k]).any():\n",
    "                                real_true_num += 1\n",
    "                            else:\n",
    "                                fake_true_num += 1\n",
    "                        if len_rate <= 1.1:\n",
    "                            len_10_num += 1\n",
    "\n",
    "            error_rate = error_num/(trajectory.size(0)*trajectory.size(1))\n",
    "            real_true_rate = real_true_num/(trajectory.size(0)*trajectory.size(1))\n",
    "            fake_true_rate = fake_true_num/(trajectory.size(0)*trajectory.size(1))\n",
    "            length_rate_avg = len_rate_sum/(trajectory.size(0)*trajectory.size(1)-error_num) if error_num != trajectory.size(0)*trajectory.size(1) else 0\n",
    "            len_10_rate = len_10_num/(trajectory.size(0)*trajectory.size(1)-error_num) if error_num != trajectory.size(0)*trajectory.size(1) else 0\n",
    "            \n",
    "            result_text += f'  Test Loss: {loss.item():<10.8f}  |  Test Acc: {acc.item():<7.2%}  |  Error Rate: {error_rate:<7.2%}  |  Real True Rate: {real_true_rate:<7.2%}  |  Fake True Rate: {fake_true_rate:<7.2%}  |  Length Rate Avg: {length_rate_avg:<7.2%}  |  Length 110% rate: {len_10_rate:<7.2%}  |'\n",
    "            print(result_text)\n",
    "            logger_test_loss.append(loss.item())\n",
    "            logger_test_error_rate.append(error_rate)\n",
    "            logger_test_real_true_rate.append(real_true_rate)\n",
    "            logger_test_fake_true_rate.append(fake_true_rate)\n",
    "            logger_test_length_rate_avg.append(length_rate_avg)\n",
    "            logger_test_leng_10_rate.append(len_10_rate)\n",
    "            \n",
    "        if i % save_iters == 0:\n",
    "            torch.save(model.state_dict(), f'./checkpoint/model_{i}.pth')\n",
    "            print(f'Model saved at iteration {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the logger curves\n",
    "\n",
    "visualization_num = int(len(logger_test_loss)*0.1)\n",
    "\n",
    "plt.figure(figsize=(30, 4))\n",
    "plt.subplot(1,7,1)\n",
    "plt.plot(logger_train_loss)\n",
    "train_loss_last = np.mean(logger_train_loss[-visualization_num:])\n",
    "plt.title('Train Loss\\n Last 10% mean: {:.8f}'.format(train_loss_last))\n",
    "\n",
    "plt.subplot(1,7,2)\n",
    "plt.plot(logger_test_loss)\n",
    "test_loss_last = np.mean(logger_test_loss[-visualization_num:])\n",
    "plt.title('Test Loss\\n Last 10% mean: {:.8f}'.format(test_loss_last))\n",
    "\n",
    "plt.subplot(1,7,3)\n",
    "plt.plot(logger_test_error_rate)\n",
    "error_rate_last = np.mean(logger_test_error_rate[-visualization_num:])\n",
    "plt.title('Error Rate\\n Last 10% mean: {:.2f}'.format(error_rate_last))\n",
    "\n",
    "plt.subplot(1,7,4)\n",
    "plt.plot(logger_test_real_true_rate)\n",
    "real_true_rate_last = np.mean(logger_test_real_true_rate[-visualization_num:])\n",
    "plt.title('Real True Rate\\n Last 10% mean: {:.2f}'.format(real_true_rate_last))\n",
    "\n",
    "plt.subplot(1,7,5)\n",
    "plt.plot(logger_test_fake_true_rate)\n",
    "fake_true_rate_last = np.mean(logger_test_fake_true_rate[-visualization_num:])\n",
    "plt.title('Fake True Rate\\n Last 10% mean: {:.2f}'.format(fake_true_rate_last))\n",
    "\n",
    "plt.subplot(1,7,6)\n",
    "plt.plot(logger_test_length_rate_avg)\n",
    "length_rate_avg_last = np.mean(logger_test_length_rate_avg[-visualization_num:])\n",
    "plt.title('Length Rate Avg\\n Last 10% mean: {:.2f}'.format(length_rate_avg_last))\n",
    "\n",
    "plt.subplot(1,7,7)\n",
    "plt.plot(logger_test_leng_10_rate)\n",
    "len_10_rate_last = np.mean(logger_test_leng_10_rate[-visualization_num:])\n",
    "plt.title('Length within 110% rate\\n Last 10% mean: {:.2f}'.format(len_10_rate_last))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate the trajectory from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "test_num = 100\n",
    "total_trajectories = 5\n",
    "test_trajectory, test_weighted_adj, test_adj_table, test_condition, special_mask = loader.generate_batch(test_num, total_trajectories)\n",
    "    \n",
    "model=model.eval()\n",
    "\n",
    "start_time = time.time()\n",
    "logits = model.test(test_condition.to(device), test_adj_table.to(device))\n",
    "print('model Time:', time.time()-start_time)\n",
    "\n",
    "predic_trajectory = logits.cpu().numpy()\n",
    "test_trajectory = test_trajectory.cpu().numpy()\n",
    "test_adj_table = test_adj_table.cpu().numpy()\n",
    "test_condition = test_condition.cpu().numpy()\n",
    "test_weighted_adj = test_weighted_adj.cpu().numpy()\n",
    "# test_trajectory: [test_num, car_num, block_size]\n",
    "# predic_trajectories: [test_num, car_num, block_size]\n",
    "# test_weighted_adj: [test_num, grid_size*grid_size, grid_size*grid_size]\n",
    "\n",
    "# compare the time between no_diffusion and dijkstra\n",
    "time_len = 0\n",
    "for i in range(test_num):\n",
    "    for j in range(total_trajectories):\n",
    "        G = transfer_graph(test_weighted_adj[i], grid_size)\n",
    "        start = test_condition[i,j,0]\n",
    "        end = test_condition[i,j,1]\n",
    "        start_time = time.time()\n",
    "        # nx.dijkstra_path_length(G, start-1, end-1, weight='weight')\n",
    "        nx.shortest_path(G, start-1, end-1, weight='weight')\n",
    "        time_len += time.time()-start_time\n",
    "print('Dijkstra Time:', time_len)\n",
    "\n",
    "def print_trajectory(predict_trajectory, test_trajectory):\n",
    "    for i in range(len(predict_trajectory)):\n",
    "        result_txt = f\"weighted graph {i+1:<3}, \"\n",
    "        for j in range(len(predict_trajectory[i])):\n",
    "            result_txt_predict = result_txt+f\"pred {j+1:<3}: {predict_trajectory[i][j]}\"\n",
    "            print(result_txt_predict)\n",
    "            \n",
    "            result_txt_test = result_txt+f\"true {j+1:<3}: {test_trajectory[i][j]}\"\n",
    "            print(result_txt_test)\n",
    "        \n",
    "\n",
    "# remove the special token, input one traj as numpy, return a list without special token\n",
    "def remove_special_token(trajectory):\n",
    "    new_trajectory = []\n",
    "    for i in range(len(trajectory)):\n",
    "        if trajectory[i] == 0:\n",
    "            break\n",
    "        new_trajectory.append(trajectory[i]-1) # 0-indexing\n",
    "    return new_trajectory\n",
    "\n",
    "\n",
    "# transfer node, wrighted_adj to graph\n",
    "def transfer_graph(weighted_adj, grid_size):\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(grid_size*grid_size):\n",
    "        G.add_node(i)\n",
    "    for i in range(grid_size*grid_size):\n",
    "        for j in range(grid_size*grid_size):\n",
    "            if weighted_adj[i,j] != 0:\n",
    "                G.add_edge(i,j,weight=weighted_adj[i,j])\n",
    "    return G\n",
    "\n",
    "\n",
    "# transfer grid into position\n",
    "def transfer_position(grid_size):\n",
    "    pos = {}\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            pos[i*grid_size+j] = (i,j)\n",
    "    return pos\n",
    "\n",
    "# Function to calculate the bounds of the graph\n",
    "def calculate_bounds(G, pos):\n",
    "    x_values = [pos[node][0] for node in G]\n",
    "    y_values = [pos[node][1] for node in G]\n",
    "    x_min, x_max = min(x_values), max(x_values)\n",
    "    y_min, y_max = min(y_values), max(y_values)\n",
    "    x_min, x_max = x_min - 0.1 * abs(x_max - x_min), x_max + 0.1 * abs(x_max - x_min) \n",
    "    y_min, y_max = y_min - 0.1 * abs(y_max - y_min), y_max + 0.1 * abs(y_max - y_min) \n",
    "\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "# Calculate the length of a trajectory\n",
    "def calculate_length(trajectory, weighted_adj):\n",
    "    length = 0\n",
    "    for i in range(len(trajectory)-1):\n",
    "        length += weighted_adj[trajectory[i]-1,trajectory[i+1]-1]\n",
    "    return length\n",
    "\n",
    "\n",
    "# Plot the trajectories on the graph\n",
    "def plot_trajs(ax, G, pos, weighted_adj, traj, traj_ = None, ground_truth=False):\n",
    "\n",
    "    if ax is None:\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=100, node_color='gray')\n",
    "        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray')\n",
    "    else:\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=100, node_color='gray',ax=ax)\n",
    "        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)\n",
    "        # nx.draw_networkx_edge_labels(G, pos, edge_labels={(i, j): f'{weighted_adj[i, j]:.2f}' for i, j in G.edges()}, label_pos=0.7, ax=ax)\n",
    "        \n",
    "\n",
    "    # Plot trajectories\n",
    "    x = [pos[node][0]+np.random.normal(0,0.01) for node in traj]\n",
    "    y = [pos[node][1]+np.random.normal(0,0.01) for node in traj]\n",
    "    traj_len = calculate_length(traj, weighted_adj)\n",
    "    ax.plot(x, y, marker='o', linewidth = 2.0, alpha=0.5, markersize=10, label=f'Generated Trajectory {traj_len}', color='blue')\n",
    "    ax.plot(x[0], y[0], marker='*', markersize=20, color='black')\n",
    "\n",
    "    if traj_ is not None:\n",
    "        x = [pos[node][0]+np.random.normal(0,0.01) for node in traj_]\n",
    "        y = [pos[node][1]+np.random.normal(0,0.01) for node in traj_]\n",
    "        traj_len = calculate_length(traj_, weighted_adj)\n",
    "        ax.plot(x, y, marker='o',  linewidth = 2.0, alpha=0.5, markersize=10, label=f'Ground Truth Trajectory {traj_len}', color='red')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "# Plot the training and validation losses\n",
    "def plot_losses(train_losses, val_losses, eval_interval):\n",
    "    x = range(0, len(train_losses)*eval_interval, eval_interval)\n",
    "    plt.plot(x, train_losses, label='train')\n",
    "    plt.plot(x, val_losses, label='val')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the trajectory\n",
    "\n",
    "print(test_trajectory.shape)\n",
    "print_trajectory(predic_trajectory, test_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trajectory\n",
    "\n",
    "fig_size = 10\n",
    "plot_num = min(5, test_num)\n",
    "\n",
    "for i in range(plot_num):\n",
    "    fig, ax = plt.subplots(1, total_trajectories, figsize=((fig_size+1)*total_trajectories, fig_size))\n",
    "    for j in range(total_trajectories):\n",
    "        traj = predic_trajectory[i,j] # (block_size,)\n",
    "        traj_ = test_trajectory[i,j] # (block_size,)\n",
    "        traj = remove_special_token(traj)\n",
    "        traj_ = remove_special_token(traj_)\n",
    "\n",
    "        G = transfer_graph(test_weighted_adj[i], grid_size)\n",
    "        pos = transfer_position(grid_size)\n",
    "        if total_trajectories == 1:\n",
    "            plot_trajs(ax, G, pos, test_weighted_adj[i], traj, traj_, ground_truth=False)\n",
    "        else:\n",
    "            plot_trajs(ax[j], G, pos, test_weighted_adj[i], traj, traj_, ground_truth=False)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
